{
  "openapi": "3.0.1",
  "info": {
    "title": "vscode-openai-inference-v1",
    "description": "Azure OpenAI APIs for completions and search",
    "version": "1.0"
  },
  "servers": [
    {
      "url": "https://api.arbs.io/openai/inference/v1"
    }
  ],
  "paths": {
    "/deployments/{deployment-id}/completions": {
      "post": {
        "summary": "Creates a completion for the provided prompt, parameters and chosen model.",
        "description": "Creates a completion for the provided prompt, parameters and chosen model.",
        "operationId": "Completions_Create",
        "parameters": [
          {
            "name": "deployment-id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "description": "Deployment id of the model which was deployed.",
              "example": "davinci"
            }
          },
          {
            "name": "api-version",
            "in": "query",
            "schema": {
              "type": "string",
              "description": "api version",
              "example": "2023-03-15-preview"
            }
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "prompt": {
                    "oneOf": [
                      {
                        "type": "string",
                        "default": "",
                        "nullable": true,
                        "example": "This is a test."
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "string",
                          "default": "",
                          "example": "This is a test."
                        },
                        "description": "Array size minimum of 1 and maximum of 2048"
                      }
                    ],
                    "description": "The prompt(s) to generate completions for, encoded as a string or array of strings.\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. Maximum allowed size of string list is 2048."
                  },
                  "max_tokens": {
                    "type": "integer",
                    "description": "The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Has minimum of 0.",
                    "default": 16,
                    "nullable": true,
                    "example": 16
                  },
                  "temperature": {
                    "type": "number",
                    "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.\nWe generally recommend altering this or top_p but not both.",
                    "default": 1,
                    "nullable": true,
                    "example": 1
                  },
                  "top_p": {
                    "type": "number",
                    "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both.",
                    "default": 1,
                    "nullable": true,
                    "example": 1
                  },
                  "logit_bias": {
                    "type": "object",
                    "description": "Defaults to null. Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {\"50256\" &#58; -100} to prevent the <|endoftext|> token from being generated."
                  },
                  "user": {
                    "type": "string",
                    "description": "A unique identifier representing your end-user, which can help monitoring and detecting abuse"
                  },
                  "n": {
                    "type": "integer",
                    "description": "How many completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed.\nNote: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.",
                    "default": 1,
                    "nullable": true,
                    "example": 1
                  },
                  "stream": {
                    "type": "boolean",
                    "description": "Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.",
                    "default": false,
                    "nullable": true
                  },
                  "logprobs": {
                    "type": "integer",
                    "description": "Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.\nMinimum of 0 and maximum of 5 allowed.",
                    "default": null,
                    "nullable": true
                  },
                  "model": {
                    "type": "string",
                    "description": "ID of the model to use. You can use the Models_List operation to see all of your available models, or see our Models_Get overview for descriptions of them.",
                    "nullable": true,
                    "example": "davinci"
                  },
                  "suffix": {
                    "type": "string",
                    "description": "The suffix that comes after a completion of inserted text.",
                    "nullable": true
                  },
                  "echo": {
                    "type": "boolean",
                    "description": "Echo back the prompt in addition to the completion",
                    "default": false,
                    "nullable": true
                  },
                  "stop": {
                    "oneOf": [
                      {
                        "type": "string",
                        "default": "<|endoftext|>",
                        "nullable": true,
                        "example": "\n"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "string",
                          "example": ["\n"]
                        },
                        "description": "Array minimum size of 1 and maximum of 4"
                      }
                    ],
                    "description": "Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."
                  },
                  "completion_config": {
                    "type": "string",
                    "nullable": true
                  },
                  "cache_level": {
                    "type": "integer",
                    "description": "can be used to disable any server-side caching, 0=no cache, 1=prompt prefix enabled, 2=full cache",
                    "nullable": true
                  },
                  "presence_penalty": {
                    "type": "number",
                    "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
                    "default": 0
                  },
                  "frequency_penalty": {
                    "type": "number",
                    "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
                    "default": 0
                  },
                  "best_of": {
                    "type": "integer",
                    "description": "Generates best_of completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.\nWhen used with n, best_of controls the number of candidate completions and n specifies how many to return â€“ best_of must be greater than n.\nNote: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. Has maximum value of 128."
                  }
                }
              },
              "example": {
                "prompt": "Negate the following sentence.The price for bubblegum increased on thursday.\n\n Negated Sentence:",
                "max_tokens": 50
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "OK",
            "headers": {
              "apim-request-id": {
                "description": "Request ID for troubleshooting purposes",
                "schema": {
                  "type": "string"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "required": ["id", "object", "created", "model", "choices"],
                  "type": "object",
                  "properties": {
                    "id": {
                      "type": "string"
                    },
                    "object": {
                      "type": "string"
                    },
                    "created": {
                      "type": "integer"
                    },
                    "model": {
                      "type": "string"
                    },
                    "choices": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "text": {
                            "type": "string"
                          },
                          "index": {
                            "type": "integer"
                          },
                          "logprobs": {
                            "type": "object",
                            "properties": {
                              "tokens": {
                                "type": "array",
                                "items": {
                                  "type": "string"
                                }
                              },
                              "token_logprobs": {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              "top_logprobs": {
                                "type": "array",
                                "items": {
                                  "type": "object",
                                  "additionalProperties": {
                                    "type": "number"
                                  }
                                }
                              },
                              "text_offset": {
                                "type": "array",
                                "items": {
                                  "type": "integer"
                                }
                              }
                            }
                          },
                          "finish_reason": {
                            "type": "string"
                          }
                        }
                      }
                    },
                    "usage": {
                      "required": [
                        "prompt_tokens",
                        "total_tokens",
                        "completion_tokens"
                      ],
                      "type": "object",
                      "properties": {
                        "completion_tokens": {
                          "type": "number",
                          "format": "int32"
                        },
                        "prompt_tokens": {
                          "type": "number",
                          "format": "int32"
                        },
                        "total_tokens": {
                          "type": "number",
                          "format": "int32"
                        }
                      }
                    }
                  }
                },
                "example": {
                  "model": "davinci",
                  "object": "text_completion",
                  "id": "cmpl-4509KAos68kxOqpE2uYGw81j6m7uo",
                  "created": 1637097562,
                  "choices": [
                    {
                      "index": 0,
                      "text": "The price for bubblegum decreased on thursday.",
                      "logprobs": null,
                      "finish_reason": "stop"
                    }
                  ]
                }
              }
            }
          },
          "400": {
            "description": "Service unavailable",
            "headers": {
              "apim-request-id": {
                "description": "Request ID for troubleshooting purposes",
                "schema": {
                  "type": "string"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/errorResponse"
                },
                "example": {
                  "error": {
                    "code": "string",
                    "message": "string",
                    "param": "string",
                    "type": "string"
                  }
                }
              }
            }
          },
          "500": {
            "description": "Service unavailable",
            "headers": {
              "apim-request-id": {
                "description": "Request ID for troubleshooting purposes",
                "schema": {
                  "type": "string"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/errorResponse"
                },
                "example": {
                  "error": {
                    "code": "string",
                    "message": "string",
                    "param": "string",
                    "type": "string"
                  }
                }
              }
            }
          }
        }
      }
    },
    "/deployments/{deployment-id}/embeddings": {
      "post": {
        "summary": "Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.",
        "description": "Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.",
        "operationId": "embeddings_create",
        "parameters": [
          {
            "name": "deployment-id",
            "in": "path",
            "description": "The deployment id of the model which was deployed.",
            "required": true,
            "schema": {
              "type": "string",
              "example": "ada-search-index-v1"
            }
          },
          {
            "name": "api-version",
            "in": "query",
            "schema": {
              "type": "string",
              "description": "api version",
              "example": "2023-03-15-preview"
            }
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "required": ["input"],
                "type": "object",
                "properties": {
                  "input": {
                    "oneOf": [
                      {
                        "type": "string",
                        "default": "",
                        "nullable": true,
                        "example": "This is a test."
                      },
                      {
                        "maxItems": 2048,
                        "minItems": 1,
                        "type": "array",
                        "items": {
                          "minLength": 1,
                          "type": "string",
                          "example": "This is a test."
                        }
                      }
                    ],
                    "description": "Input text to get embeddings for, encoded as a string. To get embeddings for multiple inputs in a single request, pass an array of strings. Each input must not exceed 2048 tokens in length.\nUnless you are embedding code, we suggest replacing newlines (\\n) in your input with a single space, as we have observed inferior results when newlines are present."
                  },
                  "user": {
                    "type": "string",
                    "description": "A unique identifier representing your end-user, which can help monitoring and detecting abuse."
                  },
                  "input_type": {
                    "type": "string",
                    "description": "input type of embedding search to use",
                    "example": "query"
                  },
                  "model": {
                    "type": "string",
                    "description": "ID of the model to use. You can use the Models_List operation to see all of your available models, or see our Models_Get overview for descriptions of them."
                  }
                }
              },
              "example": {
                "input": {},
                "user": "string",
                "input_type": "query",
                "model": "string"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "OK",
            "content": {
              "application/json": {
                "schema": {
                  "required": ["object", "model", "data", "usage"],
                  "type": "object",
                  "properties": {
                    "object": {
                      "type": "string"
                    },
                    "model": {
                      "type": "string"
                    },
                    "data": {
                      "type": "array",
                      "items": {
                        "required": ["index", "object", "embedding"],
                        "type": "object",
                        "properties": {
                          "index": {
                            "type": "integer"
                          },
                          "object": {
                            "type": "string"
                          },
                          "embedding": {
                            "type": "array",
                            "items": {
                              "type": "number"
                            }
                          }
                        }
                      }
                    },
                    "usage": {
                      "required": ["prompt_tokens", "total_tokens"],
                      "type": "object",
                      "properties": {
                        "prompt_tokens": {
                          "type": "integer"
                        },
                        "total_tokens": {
                          "type": "integer"
                        }
                      }
                    }
                  }
                },
                "example": {
                  "object": "string",
                  "model": "string",
                  "data": [
                    {
                      "index": 0,
                      "object": "string",
                      "embedding": [0]
                    }
                  ],
                  "usage": {
                    "prompt_tokens": 0,
                    "total_tokens": 0
                  }
                }
              }
            }
          }
        }
      }
    },
    "/deployments/{deployment-id}/chat/completions": {
      "post": {
        "summary": "Creates a completion for the chat message",
        "description": "Creates a completion for the chat message",
        "operationId": "ChatCompletions_Create",
        "parameters": [
          {
            "name": "deployment-id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "description": "Deployment id of the model which was deployed."
            }
          },
          {
            "name": "api-version",
            "in": "query",
            "schema": {
              "type": "string",
              "description": "api version",
              "example": "2023-03-15-preview"
            }
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "required": ["messages"],
                "type": "object",
                "properties": {
                  "messages": {
                    "minItems": 1,
                    "type": "array",
                    "items": {
                      "required": ["role", "content"],
                      "type": "object",
                      "properties": {
                        "role": {
                          "enum": ["system", "user", "assistant"],
                          "type": "string",
                          "description": "The role of the author of this message."
                        },
                        "content": {
                          "type": "string",
                          "description": "The contents of the message"
                        },
                        "name": {
                          "type": "string",
                          "description": "The name of the user in a multi-user chat"
                        }
                      }
                    },
                    "description": "The messages to generate chat completions for, in the chat format."
                  },
                  "temperature": {
                    "maximum": 2,
                    "minimum": 0,
                    "type": "number",
                    "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or `top_p` but not both.",
                    "default": 1,
                    "nullable": true,
                    "example": 1
                  },
                  "top_p": {
                    "maximum": 1,
                    "minimum": 0,
                    "type": "number",
                    "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or `temperature` but not both.",
                    "default": 1,
                    "nullable": true,
                    "example": 1
                  },
                  "n": {
                    "maximum": 128,
                    "minimum": 1,
                    "type": "integer",
                    "description": "How many chat completion choices to generate for each input message.",
                    "default": 1,
                    "nullable": true,
                    "example": 1
                  },
                  "stream": {
                    "type": "boolean",
                    "description": "If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.",
                    "default": false,
                    "nullable": true
                  },
                  "stop": {
                    "oneOf": [
                      {
                        "type": "string",
                        "nullable": true
                      },
                      {
                        "maxItems": 4,
                        "minItems": 1,
                        "type": "array",
                        "items": {
                          "type": "string"
                        },
                        "description": "Array minimum size of 1 and maximum of 4"
                      }
                    ],
                    "description": "Up to 4 sequences where the API will stop generating further tokens.",
                    "default": null
                  },
                  "max_tokens": {
                    "type": "integer",
                    "description": "The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).",
                    "default": "inf"
                  },
                  "presence_penalty": {
                    "maximum": 2,
                    "minimum": -2,
                    "type": "number",
                    "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
                    "default": 0
                  },
                  "frequency_penalty": {
                    "maximum": 2,
                    "minimum": -2,
                    "type": "number",
                    "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
                    "default": 0
                  },
                  "logit_bias": {
                    "type": "object",
                    "description": "Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.",
                    "nullable": true
                  },
                  "user": {
                    "type": "string",
                    "description": "A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse.",
                    "example": "user-1234"
                  }
                }
              },
              "example": {
                "model": "gpt-35-turbo",
                "messages": [
                  {
                    "role": "user",
                    "content": "Hello!"
                  }
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "OK",
            "content": {
              "application/json": {
                "schema": {
                  "required": ["id", "object", "created", "model", "choices"],
                  "type": "object",
                  "properties": {
                    "id": {
                      "type": "string"
                    },
                    "object": {
                      "type": "string"
                    },
                    "created": {
                      "type": "integer",
                      "format": "unixtime"
                    },
                    "model": {
                      "type": "string"
                    },
                    "choices": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "index": {
                            "type": "integer"
                          },
                          "message": {
                            "required": ["role", "content"],
                            "type": "object",
                            "properties": {
                              "role": {
                                "enum": ["system", "user", "assistant"],
                                "type": "string",
                                "description": "The role of the author of this message."
                              },
                              "content": {
                                "type": "string",
                                "description": "The contents of the message"
                              }
                            }
                          },
                          "finish_reason": {
                            "type": "string"
                          }
                        }
                      }
                    },
                    "usage": {
                      "required": [
                        "prompt_tokens",
                        "completion_tokens",
                        "total_tokens"
                      ],
                      "type": "object",
                      "properties": {
                        "prompt_tokens": {
                          "type": "integer"
                        },
                        "completion_tokens": {
                          "type": "integer"
                        },
                        "total_tokens": {
                          "type": "integer"
                        }
                      }
                    }
                  }
                },
                "example": {
                  "id": "chatcmpl-123",
                  "object": "chat.completion",
                  "created": 1677652288,
                  "choices": [
                    {
                      "index": 0,
                      "message": {
                        "role": "assistant",
                        "content": "\n\nHello there, how may I assist you today?"
                      },
                      "finish_reason": "stop"
                    }
                  ],
                  "usage": {
                    "prompt_tokens": 9,
                    "completion_tokens": 12,
                    "total_tokens": 21
                  }
                }
              }
            }
          }
        }
      }
    },
    "/models": {
      "get": {
        "summary": "Get Models",
        "operationId": "get-models",
        "responses": {
          "200": {
            "description": null
          }
        }
      }
    }
  },
  "components": {
    "schemas": {
      "errorResponse": {
        "type": "object",
        "properties": {
          "error": {
            "type": "object",
            "properties": {
              "code": {
                "type": "string"
              },
              "message": {
                "type": "string"
              },
              "param": {
                "type": "string"
              },
              "type": {
                "type": "string"
              }
            }
          }
        }
      }
    },
    "securitySchemes": {
      "apiKeyHeader": {
        "type": "apiKey",
        "name": "Ocp-Apim-Subscription-Key",
        "in": "header"
      },
      "apiKeyQuery": {
        "type": "apiKey",
        "name": "subscription-key",
        "in": "query"
      }
    }
  },
  "security": [
    {
      "apiKeyHeader": []
    },
    {
      "apiKeyQuery": []
    }
  ]
}
